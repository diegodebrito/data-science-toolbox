{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Fold for Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cv_2folds(X):\n",
    "    n = X.shape[0]\n",
    "    i = 1\n",
    "    while i<=2:\n",
    "        idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)\n",
    "        yield idx, idx\n",
    "        i += 1\n",
    "custom_cv = custom_cv_2folds(X)\n",
    "cross_val_score(clf, X, y, cv=custom_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search + Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=Pipeline([\n",
    "          ('reduce_dim', PCA()), \n",
    "          ('clf', SVC())     \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of __ for grid search using a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'reduce_dim__n_components':[2, 5, 10],\n",
    "    'clf__C':[0.1, 10, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more parameter options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(reduce_dim=['passthrough', PCA(5), PCA(10)],\n",
    "                   clf=[SVC(), LogisticRegression()],\n",
    "                   clf__C=[0.1, 10, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization (Sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Preprocessed Data (check the house prices notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:30:34.187935Z",
     "start_time": "2020-04-08T17:30:34.128866Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/houses_preprocessed_train.csv').iloc[:, 1:]\n",
    "y = pd.read_csv('./data/houses_log_y.csv')['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T02:06:51.067449Z",
     "start_time": "2020-04-08T02:06:50.555483Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nrounds_xgb(model, X, y, metrics='rmse',\n",
    "                     cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    xgtrain = xgb.DMatrix(X, y)\n",
    "    \n",
    "    params = model.get_xgb_params()\n",
    "    \n",
    "    cvresult = xgb.cv(params, xgtrain, metrics=metrics, \n",
    "                      num_boost_round=params['n_estimators'],\n",
    "                      early_stopping_rounds=early_stopping_rounds)\n",
    "    \n",
    "    # Setting optimal number of estimators\n",
    "    n_rounds_optimal = cvresult.shape[0]\n",
    "    model.set_params(n_estimators=n_rounds_optimal)\n",
    "    \n",
    "    print(cvresult.iloc[-1, :])\n",
    "    print(f\"n_estimators:{n_rounds_optimal}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(estimator, X, y, params, scoring, cv=4, random=True,\n",
    "                n_iter=150, n_jobs=6):\n",
    "    \n",
    "    if random:\n",
    "        random_search = RandomizedSearchCV(estimator, \n",
    "                                           param_distributions=params,\n",
    "                                           n_iter=n_iter, n_jobs=n_jobs, \n",
    "                                           cv=cv, scoring=scoring,\n",
    "                                           verbose=3, random_state=340)\n",
    "    \n",
    "    else:\n",
    "        random_search = GridSearchCV(estimator, param_grid=params, \n",
    "                                       n_jobs=n_jobs, cv=cv,\n",
    "                                       scoring='neg_mean_absolute_error',\n",
    "                                       verbose=3)\n",
    "\n",
    "    random_search.fit(X, y)\n",
    "    \n",
    "    return random_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar to the procedure used below for Lightgbm. Check the house prediction notebook (on tabular-data-techniques) for more details. The order of hyperparameter optimization used was the following:  \n",
    "- best number of estimators for a baseline model\n",
    "- max_depth and min_child_weight  \n",
    "- gamma  \n",
    "- subsample and colsample_bytree\n",
    "- adjust number of estimator and learning rate (usually multiplying by 10 and dividing by 10 respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:42:41.229781Z",
     "start_time": "2020-04-08T16:42:41.225779Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:54:43.208544Z",
     "start_time": "2020-04-08T15:54:43.201514Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_nrounds_lgb(model, X, y, metrics='rmse',\n",
    "                     cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    lgbtrain = lgb.Dataset(X, y)\n",
    "    \n",
    "    params = model.get_params()\n",
    "    \n",
    "    # ATTENTION: we need to set stratified to False for regression\n",
    "    cvresult = lgb.cv(params, lgbtrain, metrics=metrics, \n",
    "                      num_boost_round=params['n_estimators'],\n",
    "                      early_stopping_rounds=early_stopping_rounds,\n",
    "                      stratified=False)\n",
    "    \n",
    "    # Setting optimal number of estimators\n",
    "    key = list(cvresult.keys())[0]\n",
    "    n_rounds_optimal = len(cvresult[key])\n",
    "    model.set_params(n_estimators=n_rounds_optimal)\n",
    "    \n",
    "    print(cvresult[key][-1])\n",
    "    print(f\"n_estimators:{n_rounds_optimal}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:41:56.009220Z",
     "start_time": "2020-04-08T16:41:56.004212Z"
    }
   },
   "outputs": [],
   "source": [
    "def grid_search(estimator, X, y, params, scoring, cv=4, random=True,\n",
    "                n_iter=150, n_jobs=6):\n",
    "    \n",
    "    if random:\n",
    "        random_search = RandomizedSearchCV(estimator, \n",
    "                                           param_distributions=params,\n",
    "                                           n_iter=n_iter, n_jobs=n_jobs, \n",
    "                                           cv=cv, scoring=scoring,\n",
    "                                           verbose=3, random_state=340)\n",
    "    \n",
    "    else:\n",
    "        random_search = GridSearchCV(estimator, param_grid=params, \n",
    "                                       n_jobs=n_jobs, cv=cv,\n",
    "                                       scoring='neg_mean_absolute_error',\n",
    "                                       verbose=3)\n",
    "\n",
    "    random_search.fit(X, y)\n",
    "    \n",
    "    return random_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM uses leaf-wise tree growth algorithm. For this reason, limiting the number of leaves instead of max_depth might be more appropriate. Like we did when tuning XGBoost, we will start by finding the number of estimators using cross validation and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:38:51.033030Z",
     "start_time": "2020-04-08T16:38:51.029030Z"
    }
   },
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(learning_rate=0.1, n_estimators=1000,\n",
    "                          num_leaves=31, min_child_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:38:54.060556Z",
     "start_time": "2020-04-08T16:38:51.956560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12324168104250295\n",
      "n_estimators:74\n"
     ]
    }
   ],
   "source": [
    "model = find_nrounds_lgb(model, train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we find the **number of leaves**. The dataset is quite small, so we will start searching on low values. Let's vary **maximum depth** as well and see if that helps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:57:08.758988Z",
     "start_time": "2020-04-08T16:56:46.165571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 80 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=6)]: Done 276 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=6)]: Done 320 out of 320 | elapsed:   22.3s finished\n"
     ]
    }
   ],
   "source": [
    "param_test = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'num_leaves':range(1,41,2)\n",
    "}\n",
    "\n",
    "results = grid_search(model, train, y, param_test,\n",
    "                      scoring='neg_mean_squared_error', \n",
    "                      random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:57:20.621440Z",
     "start_time": "2020-04-08T16:57:20.614441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.08009788273854973\n",
      "{'max_depth': 9, 'num_leaves': 13}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {-results.best_score_}\")\n",
    "print(results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:57:27.636126Z",
     "start_time": "2020-04-08T16:57:27.631127Z"
    }
   },
   "outputs": [],
   "source": [
    "model.set_params(**results.best_params_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of min_child_weight like in XGBoost, let's tune **min_data_in_leaf**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:58:50.964610Z",
     "start_time": "2020-04-08T16:58:46.492645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 19 candidates, totalling 76 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=6)]: Done  76 out of  76 | elapsed:    4.2s finished\n"
     ]
    }
   ],
   "source": [
    "param_test = {\n",
    " 'min_data_in_leaf':range(5,100,5)\n",
    "}\n",
    "\n",
    "results = grid_search(model, train, y, param_test,\n",
    "                      scoring='neg_mean_squared_error', \n",
    "                      random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:58:53.170891Z",
     "start_time": "2020-04-08T16:58:53.164893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.08009788273854973\n",
      "{'min_data_in_leaf': 20}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {-results.best_score_}\")\n",
    "print(results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:58:58.716292Z",
     "start_time": "2020-04-08T16:58:58.712289Z"
    }
   },
   "outputs": [],
   "source": [
    "model.set_params(**results.best_params_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike XGBoost, we don't have a **gamma** parameter on LightGBM. The similar parameter here is **min_split_gain**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:59:37.726537Z",
     "start_time": "2020-04-08T16:59:30.352378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 51 candidates, totalling 204 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=6)]: Done 204 out of 204 | elapsed:    7.1s finished\n"
     ]
    }
   ],
   "source": [
    "param_test = {\n",
    " 'min_split_gain':[i/10.0 for i in range(0,51)]\n",
    "}\n",
    "\n",
    "results = grid_search(model, train, y, param_test,\n",
    "                      scoring='neg_mean_squared_error', \n",
    "                      random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:59:38.876032Z",
     "start_time": "2020-04-08T16:59:38.870029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.08009788273854973\n",
      "{'min_split_gain': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {-results.best_score_}\")\n",
    "print(results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:00:15.844318Z",
     "start_time": "2020-04-08T17:00:15.839318Z"
    }
   },
   "outputs": [],
   "source": [
    "model.set_params(**results.best_params_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:01:00.327627Z",
     "start_time": "2020-04-08T17:01:00.319636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.1, max_depth=9,\n",
       "              min_child_samples=20, min_child_weight=1, min_data_in_leaf=20,\n",
       "              min_split_gain=0.0, n_estimators=74, n_jobs=-1, num_leaves=13,\n",
       "              objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
       "              silent=True, subsample=1.0, subsample_for_bin=200000,\n",
       "              subsample_freq=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's **colsample_bytree** and **subsample**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:02:56.391472Z",
     "start_time": "2020-04-08T17:02:48.062474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 45 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=6)]: Done 180 out of 180 | elapsed:    8.1s finished\n"
     ]
    }
   ],
   "source": [
    "param_test = {\n",
    " 'subsample':[i/10.0 for i in range(5,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(1,10)]\n",
    "}\n",
    "\n",
    "results = grid_search(model, train, y, param_test,\n",
    "                      scoring='neg_mean_squared_error', \n",
    "                      random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:02:57.815505Z",
     "start_time": "2020-04-08T17:02:57.810471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.07821223535152377\n",
      "{'colsample_bytree': 0.3, 'subsample': 0.5}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {-results.best_score_}\")\n",
    "print(results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:03:03.818787Z",
     "start_time": "2020-04-08T17:03:03.813789Z"
    }
   },
   "outputs": [],
   "source": [
    "model.set_params(**results.best_params_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the learning_rate and increasing the number of estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:05:41.602325Z",
     "start_time": "2020-04-08T17:05:41.598325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=0.3,\n",
       "              importance_type='split', learning_rate=0.01, max_depth=9,\n",
       "              min_child_samples=20, min_child_weight=1, min_data_in_leaf=20,\n",
       "              min_split_gain=0.0, n_estimators=720, n_jobs=-1, num_leaves=13,\n",
       "              objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
       "              silent=True, subsample=0.5, subsample_for_bin=200000,\n",
       "              subsample_freq=0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.set_params(**{'learning_rate':0.01, 'n_estimators':720})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluating the final performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:05:51.140526Z",
     "start_time": "2020-04-08T17:05:43.653526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11346970518757978"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfolds = KFold(n_splits=10)\n",
    "np.mean(np.sqrt(-cross_val_score(model, train, y, \n",
    "                                 scoring=\"neg_mean_squared_error\",\n",
    "                                 cv=kfolds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/competitive-data-science/lecture/75oIn/catboost-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization (Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:29:21.823214Z",
     "start_time": "2020-04-08T17:29:20.494109Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nrounds_lgb(model, X, y, metrics='rmse',\n",
    "                     cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    lgbtrain = lgb.Dataset(X, y)\n",
    "    \n",
    "    params = model.get_params()\n",
    "    \n",
    "    # ATTENTION: we need to set stratified to False for regression\n",
    "    cvresult = lgb.cv(params, lgbtrain, metrics=metrics, \n",
    "                      num_boost_round=params['n_estimators'],\n",
    "                      early_stopping_rounds=early_stopping_rounds,\n",
    "                      stratified=False)\n",
    "    \n",
    "    # Setting optimal number of estimators\n",
    "    key = list(cvresult.keys())[0]\n",
    "    n_rounds_optimal = len(cvresult[key])\n",
    "    model.set_params(n_estimators=n_rounds_optimal)\n",
    "    \n",
    "    print(cvresult[key][-1])\n",
    "    print(f\"n_estimators:{n_rounds_optimal}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(estimator, X, y, params, scoring, cv=4, random=True,\n",
    "                n_iter=150, n_jobs=6):\n",
    "    \n",
    "    if random:\n",
    "        random_search = RandomizedSearchCV(estimator, \n",
    "                                           param_distributions=params,\n",
    "                                           n_iter=n_iter, n_jobs=n_jobs, \n",
    "                                           cv=cv, scoring=scoring,\n",
    "                                           verbose=3, random_state=340)\n",
    "    \n",
    "    else:\n",
    "        random_search = GridSearchCV(estimator, param_grid=params, \n",
    "                                       n_jobs=n_jobs, cv=cv,\n",
    "                                       scoring='neg_mean_absolute_error',\n",
    "                                       verbose=3)\n",
    "\n",
    "    random_search.fit(X, y)\n",
    "    \n",
    "    return random_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:30:53.359466Z",
     "start_time": "2020-04-08T17:30:53.353430Z"
    }
   },
   "source": [
    "Loading Preprocessed Data (check the house prices notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:31:03.351030Z",
     "start_time": "2020-04-08T17:31:03.287925Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/houses_preprocessed_train.csv').iloc[:, 1:]\n",
    "y = pd.read_csv('./data/houses_log_y.csv')['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the base model and finding the number of estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:31:18.365897Z",
     "start_time": "2020-04-08T17:31:18.360895Z"
    }
   },
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(learning_rate=0.1, n_estimators=1000,\n",
    "                          num_leaves=31, min_child_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:31:21.010752Z",
     "start_time": "2020-04-08T17:31:18.890796Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USUARIO\\Anaconda3\\envs\\dl\\lib\\site-packages\\lightgbm\\engine.py:503: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\USUARIO\\Anaconda3\\envs\\dl\\lib\\site-packages\\lightgbm\\basic.py:842: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
      "Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  .format(key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12324168104250295\n",
      "n_estimators:74\n"
     ]
    }
   ],
   "source": [
    "model = find_nrounds_lgb(model, train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Grid for Random Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:09:36.857797Z",
     "start_time": "2020-04-08T18:03:13.105617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 500 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=6)]: Done 276 tasks      | elapsed:   46.0s\n",
      "[Parallel(n_jobs=6)]: Done 500 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=6)]: Done 788 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=6)]: Done 1140 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=6)]: Done 1556 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=6)]: Done 2000 out of 2000 | elapsed:  6.4min finished\n"
     ]
    }
   ],
   "source": [
    "param_test = {\n",
    " 'num_leaves':range(2,42,2),\n",
    " 'subsample': np.linspace(0.5, 1, 100),\n",
    " 'colsample_bytree': np.linspace(0.5, 1, 100),\n",
    " 'min_data_in_leaf':range(4,52,2),\n",
    " 'min_split_gain':np.linspace(0.0, 0.5, 100)\n",
    "}\n",
    "\n",
    "results = grid_search(model, train, y, param_test,\n",
    "                      scoring='neg_mean_squared_error', \n",
    "                      random=True, n_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:11:47.130005Z",
     "start_time": "2020-04-08T18:11:47.125012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.013705892792869795\n",
      "{'subsample': 0.6212121212121212, 'num_leaves': 34, 'min_split_gain': 0.005050505050505051, 'min_data_in_leaf': 34, 'colsample_bytree': 0.5050505050505051}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {-results.best_score_}\")\n",
    "print(results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using best hyperparameters, reducing learning rate and increasing n_estimators proportionally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:11:49.054989Z",
     "start_time": "2020-04-08T18:11:49.050956Z"
    }
   },
   "outputs": [],
   "source": [
    "model.set_params(**results.best_params_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:11:50.178471Z",
     "start_time": "2020-04-08T18:11:50.172511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
       "              colsample_bytree=0.5050505050505051, importance_type='split',\n",
       "              learning_rate=0.01, max_depth=-1, min_child_samples=20,\n",
       "              min_child_weight=1, min_data_in_leaf=34,\n",
       "              min_split_gain=0.005050505050505051, n_estimators=740, n_jobs=-1,\n",
       "              num_leaves=34, objective=None, random_state=None, reg_alpha=0.0,\n",
       "              reg_lambda=0.0, silent=True, subsample=0.6212121212121212,\n",
       "              subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.set_params(**{'learning_rate':0.01, 'n_estimators':740})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final model performance (very close, but slightly worse than the sequential approach):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:12:02.902331Z",
     "start_time": "2020-04-08T18:11:51.861339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11596549007755608"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfolds = KFold(n_splits=10)\n",
    "np.mean(np.sqrt(-cross_val_score(model, train, y, \n",
    "                                 scoring=\"neg_mean_squared_error\",\n",
    "                                 cv=kfolds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References  \n",
    "- https://roamanalytics.com/2016/09/15/optimizing-the-hyperparameter-of-which-hyperparameter-optimizer-to-use/   \n",
    "- https://towardsdatascience.com/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T21:04:28.318648Z",
     "start_time": "2020-04-17T21:04:28.312666Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, STATUS_FAIL\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T21:04:28.377412Z",
     "start_time": "2020-04-17T21:04:28.319646Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/houses_preprocessed_train.csv').iloc[:, 1:]\n",
    "y = pd.read_csv('./data/houses_log_y.csv')['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split for the simple cases (it might be better to use cross validation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T21:04:28.386390Z",
     "start_time": "2020-04-17T21:04:28.379408Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train, y, test_size=0.3,\n",
    "                                                   random_state=498)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T14:42:34.241948Z",
     "start_time": "2020-04-17T14:42:34.069524Z"
    }
   },
   "source": [
    "Very useful class from the second reference above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T21:07:29.478257Z",
     "start_time": "2020-04-17T21:07:29.468328Z"
    }
   },
   "outputs": [],
   "source": [
    "class HPOpt():\n",
    "    \n",
    "    def __init__(self, x_train, x_test, y_train, y_test):\n",
    "        \n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def find_best(self, fn_name, space, algo, max_evals, trials):\n",
    "        \n",
    "        # Select one of the functions (XGBoost, LightGBM, Catboost)\n",
    "        fn = getattr(self, fn_name)\n",
    "        try:\n",
    "            # Optimize the chose function on space. Space in this case\n",
    "            # contains the hyperparameters and also fitting options\n",
    "            # the latter are fixes\n",
    "            result = fmin(fn=fn, space=space, algo=algo, \n",
    "                          max_evals=max_evals,\n",
    "                          trials=trials)\n",
    "        except Exception as e:\n",
    "            return {'status': STATUS_FAIL,\n",
    "                    'exception': str(e)}\n",
    "        return result, trials\n",
    "    \n",
    "    # Hyperopt will basically vary params (remember that only\n",
    "    # hyperparam_grid will effectively change)\n",
    "    # Example: the combination of xgb and train_reg return a\n",
    "    # loss value that is given to Hyperot\n",
    "    def xgb_reg(self, parameters):\n",
    "        print(parameters['hyperparam_grid'])\n",
    "        reg = xgb.XGBRegressor(**parameters['hyperparam_grid'])\n",
    "        return self.train_reg(reg, parameters)\n",
    "    \n",
    "    # Using cross validation for each parameter configuration (VERY slow)\n",
    "    def train_reg(self, reg, parameters):\n",
    "        \n",
    "        kfolds = KFold(n_splits=5)\n",
    "        results = -cross_val_score(reg, self.x_train, self.y_train,\n",
    "                                  scoring=\"neg_mean_squared_error\",\n",
    "                                  cv=kfolds)\n",
    "        print(results)\n",
    "        loss = np.mean(np.sqrt(results))\n",
    "        return {'loss': loss, 'status':STATUS_OK}\n",
    "    \n",
    "    \n",
    "#     def train_reg(self, reg, parameters):\n",
    "        \n",
    "#         reg.fit(self.x_train, self.y_train,\n",
    "#                 eval_set = [(self.x_train, self.y_train), (self.x_test, self.y_test)],\n",
    "#                 **parameters['fit_params'])\n",
    "#         pred = reg.predict(self.x_test)\n",
    "#         loss = parameters['loss_func'](self.y_test, pred)\n",
    "#         return {'loss': loss, 'status':STATUS_OK}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid of hyperparameters and fitting options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T21:07:56.076649Z",
     "start_time": "2020-04-17T21:07:56.070616Z"
    }
   },
   "outputs": [],
   "source": [
    "hyperparam_grid = {\n",
    "    'max_depth': hp.choice('max_depth', range(3,15,1)),\n",
    "    'min_child_weight': hp.choice('min_child_weight', range(1, 8, 1)),\n",
    "    'gamma': hp.quniform('gamma', 0, 0.50, 0.01),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "    'n_estimators': hp.choice('n_estimators', range(100,1100,20))\n",
    "}\n",
    "\n",
    "fit_params = {\n",
    "    'eval_metric' : 'rmse',\n",
    "    'early_stopping_rounds': 10,\n",
    "    'verbose': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consolidating everything on a dictionary, together with a loss function. This will be passed as space to hyperopt, although only hyperparam_grid will effectively be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T21:07:59.487110Z",
     "start_time": "2020-04-17T21:07:59.482131Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_parameters = dict()\n",
    "xgb_parameters['hyperparam_grid'] = hyperparam_grid\n",
    "xgb_parameters['fit_params'] = fit_params\n",
    "xgb_parameters['loss_func'] = lambda y, pred: mean_squared_error(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating and finding the best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T21:07:59.497114Z",
     "start_time": "2020-04-17T21:07:59.489103Z"
    }
   },
   "outputs": [],
   "source": [
    "hp_optimizer = HPOpt(train, _, y, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T21:14:36.907764Z",
     "start_time": "2020-04-17T21:08:00.020291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.6066805486781921, 'gamma': 0.13, 'learning_rate': 0.13843051001846368, 'max_depth': 11, 'min_child_weight': 2, 'n_estimators': 540, 'subsample': 0.6754130930019127}\n",
      "[0.01420994 0.01534662 0.01596095 0.01387783 0.0127268 ]                                                               \n",
      "{'colsample_bytree': 0.6905402242179288, 'gamma': 0.2, 'learning_rate': 0.28923528913242796, 'max_depth': 4, 'min_child_weight': 7, 'n_estimators': 900, 'subsample': 0.5274664976713547}\n",
      "[0.01474499 0.01776652 0.01763473 0.01586957 0.01360443]                                                               \n",
      "{'colsample_bytree': 0.6017211435302764, 'gamma': 0.05, 'learning_rate': 0.08891957249505401, 'max_depth': 14, 'min_child_weight': 1, 'n_estimators': 120, 'subsample': 0.6661530614258052}\n",
      "[0.01298192 0.01482931 0.01618965 0.01363572 0.01207148]                                                               \n",
      "{'colsample_bytree': 0.8733978262901544, 'gamma': 0.26, 'learning_rate': 0.19566665970803604, 'max_depth': 4, 'min_child_weight': 4, 'n_estimators': 1080, 'subsample': 0.6224343055587863}\n",
      "[0.01477162 0.01595497 0.01760889 0.01621305 0.0156208 ]                                                               \n",
      "{'colsample_bytree': 0.5317910341327807, 'gamma': 0.32, 'learning_rate': 0.1064241570081881, 'max_depth': 6, 'min_child_weight': 3, 'n_estimators': 480, 'subsample': 0.6719928571292284}\n",
      "[0.01546737 0.01716131 0.01718136 0.01676    0.01366377]                                                               \n",
      "{'colsample_bytree': 0.8471186097690782, 'gamma': 0.45, 'learning_rate': 0.0628094065068807, 'max_depth': 4, 'min_child_weight': 7, 'n_estimators': 380, 'subsample': 0.5693348518042936}\n",
      "[0.01640277 0.01835929 0.01868168 0.01655811 0.01510453]                                                               \n",
      "{'colsample_bytree': 0.5557082115644214, 'gamma': 0.01, 'learning_rate': 0.29933287327312896, 'max_depth': 7, 'min_child_weight': 4, 'n_estimators': 1060, 'subsample': 0.6406963596369235}\n",
      "[0.01437316 0.01727129 0.01919976 0.01531212 0.01276653]                                                               \n",
      "{'colsample_bytree': 0.604913337104378, 'gamma': 0.43, 'learning_rate': 0.24747063029788652, 'max_depth': 10, 'min_child_weight': 3, 'n_estimators': 220, 'subsample': 0.5870879896770601}\n",
      "[0.01715938 0.01926168 0.01913681 0.01691454 0.01452002]                                                               \n",
      "{'colsample_bytree': 0.8636060576914117, 'gamma': 0.17, 'learning_rate': 0.1016672650328126, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 780, 'subsample': 0.8326018633675409}\n",
      "[0.01434202 0.01641607 0.01550731 0.01342831 0.01324199]                                                               \n",
      "{'colsample_bytree': 0.5829497124084697, 'gamma': 0.14, 'learning_rate': 0.05738527948040528, 'max_depth': 14, 'min_child_weight': 6, 'n_estimators': 660, 'subsample': 0.8041323530241947}\n",
      "[0.01326595 0.01466238 0.01591818 0.01349189 0.01230692]                                                               \n",
      "{'colsample_bytree': 0.7449027190113552, 'gamma': 0.11, 'learning_rate': 0.6606625916853476, 'max_depth': 12, 'min_child_weight': 7, 'n_estimators': 780, 'subsample': 0.5849501963382117}\n",
      "[0.02314926 0.02760607 0.02601004 0.02510571 0.0256352 ]                                                               \n",
      "{'colsample_bytree': 0.8313452150787087, 'gamma': 0.28, 'learning_rate': 0.6034682760286911, 'max_depth': 10, 'min_child_weight': 3, 'n_estimators': 600, 'subsample': 0.6012131286079202}\n",
      "[0.02013254 0.02125268 0.0183621  0.02299234 0.01849984]                                                               \n",
      "{'colsample_bytree': 0.8578291648206159, 'gamma': 0.25, 'learning_rate': 0.16721083648625898, 'max_depth': 7, 'min_child_weight': 2, 'n_estimators': 420, 'subsample': 0.9017593588036652}\n",
      "[0.01533163 0.01585991 0.01752578 0.01505481 0.01406577]                                                               \n",
      "{'colsample_bytree': 0.8708052834270266, 'gamma': 0.19, 'learning_rate': 0.0812101536250863, 'max_depth': 13, 'min_child_weight': 2, 'n_estimators': 440, 'subsample': 0.6383031439339286}\n",
      "[0.01424033 0.01553813 0.01710408 0.01410866 0.01269644]                                                               \n",
      "{'colsample_bytree': 0.7516175467877606, 'gamma': 0.06, 'learning_rate': 0.24024749752222016, 'max_depth': 10, 'min_child_weight': 5, 'n_estimators': 300, 'subsample': 0.6703484208962847}\n",
      "[0.01599375 0.01735151 0.01674332 0.01489477 0.01350113]                                                               \n",
      "{'colsample_bytree': 0.5569101333044197, 'gamma': 0.13, 'learning_rate': 0.7962461557476468, 'max_depth': 6, 'min_child_weight': 6, 'n_estimators': 1040, 'subsample': 0.7482719352332148}\n",
      "[0.02155256 0.02773663 0.02581736 0.02208149 0.02969842]                                                               \n",
      "{'colsample_bytree': 0.9379971860812191, 'gamma': 0.24, 'learning_rate': 0.07847086222834777, 'max_depth': 13, 'min_child_weight': 4, 'n_estimators': 160, 'subsample': 0.7491342051923546}\n",
      "[0.01512473 0.01678685 0.0170966  0.01521634 0.01341617]                                                               \n",
      "{'colsample_bytree': 0.7829757729990501, 'gamma': 0.19, 'learning_rate': 0.1351240275535804, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 340, 'subsample': 0.7764353182787358}\n",
      "[0.01459589 0.01712896 0.01700541 0.01457919 0.0135611 ]                                                               \n",
      "{'colsample_bytree': 0.5317923508397746, 'gamma': 0.06, 'learning_rate': 0.05721821602081742, 'max_depth': 9, 'min_child_weight': 2, 'n_estimators': 440, 'subsample': 0.8237944860062464}\n",
      "[0.0127175  0.01426363 0.01525773 0.01230719 0.01189174]                                                               \n",
      "{'colsample_bytree': 0.6630962638475952, 'gamma': 0.41000000000000003, 'learning_rate': 0.21243693397125235, 'max_depth': 5, 'min_child_weight': 7, 'n_estimators': 820, 'subsample': 0.6823222896413068}\n",
      "[0.0179665  0.02014303 0.01954472 0.01610527 0.01623617]                                                               \n",
      "100%|███████████████████████████████████████████████| 20/20 [06:36<00:00, 16.45s/trial, best loss: 0.11514230911935294]\n"
     ]
    }
   ],
   "source": [
    "xgb_opt = hp_optimizer.find_best(fn_name='xgb_reg', \n",
    "                                 space=xgb_parameters, \n",
    "                                 trials=Trials(), algo=tpe.suggest, \n",
    "                                 max_evals=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T21:16:16.514515Z",
     "start_time": "2020-04-17T21:16:16.509529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.5317923508397746,\n",
       "  'gamma': 0.06,\n",
       "  'learning_rate': 0.05721821602081742,\n",
       "  'max_depth': 6,\n",
       "  'min_child_weight': 1,\n",
       "  'n_estimators': 17,\n",
       "  'subsample': 0.8237944860062464},\n",
       " <hyperopt.base.Trials at 0x1cab8115f60>)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T21:16:42.539649Z",
     "start_time": "2020-04-17T21:16:42.534657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.5317923508397746,\n",
       "  'gamma': 0.06,\n",
       "  'learning_rate': 0.05721821602081742,\n",
       "  'max_depth': 6,\n",
       "  'min_child_weight': 1,\n",
       "  'n_estimators': 17,\n",
       "  'subsample': 0.8237944860062464},\n",
       " <hyperopt.base.Trials at 0x1cab8115f60>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: Hyperband / Skoptimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://www.quora.com/What-methods-do-you-prefer-when-performing-hyperparameter-optimization\n",
    "\n",
    "https://github.com/zygmuntz/hyperband"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
