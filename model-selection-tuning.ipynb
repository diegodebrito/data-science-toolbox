{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Fold for Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cv_2folds(X):\n",
    "    n = X.shape[0]\n",
    "    i = 1\n",
    "    while i<=2:\n",
    "        idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)\n",
    "        yield idx, idx\n",
    "        i += 1\n",
    "custom_cv = custom_cv_2folds(X)\n",
    "cross_val_score(clf, X, y, cv=custom_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search + Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=Pipeline([\n",
    "          ('reduce_dim', PCA()), \n",
    "          ('clf', SVC())     \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of __ for grid search using a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'reduce_dim__n_components':[2, 5, 10],\n",
    "    'clf__C':[0.1, 10, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more parameter options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(reduce_dim=['passthrough', PCA(5), PCA(10)],\n",
    "                   clf=[SVC(), LogisticRegression()],\n",
    "                   clf__C=[0.1, 10, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization (Sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Preprocessed Data (check the house prices notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:30:34.187935Z",
     "start_time": "2020-04-08T17:30:34.128866Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/houses_preprocessed_train.csv').iloc[:, 1:]\n",
    "y = pd.read_csv('./data/houses_log_y.csv')['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T02:06:51.067449Z",
     "start_time": "2020-04-08T02:06:50.555483Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nrounds_xgb(model, X, y, metrics='rmse',\n",
    "                     cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    xgtrain = xgb.DMatrix(X, y)\n",
    "    \n",
    "    params = model.get_xgb_params()\n",
    "    \n",
    "    cvresult = xgb.cv(params, xgtrain, metrics=metrics, \n",
    "                      num_boost_round=params['n_estimators'],\n",
    "                      early_stopping_rounds=early_stopping_rounds)\n",
    "    \n",
    "    # Setting optimal number of estimators\n",
    "    n_rounds_optimal = cvresult.shape[0]\n",
    "    model.set_params(n_estimators=n_rounds_optimal)\n",
    "    \n",
    "    print(cvresult.iloc[-1, :])\n",
    "    print(f\"n_estimators:{n_rounds_optimal}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(estimator, X, y, params, scoring, cv=4, random=True,\n",
    "                n_iter=150, n_jobs=6):\n",
    "    \n",
    "    if random:\n",
    "        random_search = RandomizedSearchCV(estimator, \n",
    "                                           param_distributions=params,\n",
    "                                           n_iter=n_iter, n_jobs=n_jobs, \n",
    "                                           cv=cv, scoring=scoring,\n",
    "                                           verbose=3, random_state=340)\n",
    "    \n",
    "    else:\n",
    "        random_search = GridSearchCV(estimator, param_grid=params, \n",
    "                                       n_jobs=n_jobs, cv=cv,\n",
    "                                       scoring='neg_mean_absolute_error',\n",
    "                                       verbose=3)\n",
    "\n",
    "    random_search.fit(X, y)\n",
    "    \n",
    "    return random_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar to the procedure used below for Lightgbm. Check the house prediction notebook (on tabular-data-techniques) for more details. The order of hyperparameter optimization used was the following:  \n",
    "- best number of estimators for a baseline model\n",
    "- max_depth and min_child_weight  \n",
    "- gamma  \n",
    "- subsample and colsample_bytree\n",
    "- adjust number of estimator and learning rate (usually multiplying by 10 and dividing by 10 respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:42:41.229781Z",
     "start_time": "2020-04-08T16:42:41.225779Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:54:43.208544Z",
     "start_time": "2020-04-08T15:54:43.201514Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_nrounds_lgb(model, X, y, metrics='rmse',\n",
    "                     cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    lgbtrain = lgb.Dataset(X, y)\n",
    "    \n",
    "    params = model.get_params()\n",
    "    \n",
    "    # ATTENTION: we need to set stratified to False for regression\n",
    "    cvresult = lgb.cv(params, lgbtrain, metrics=metrics, \n",
    "                      num_boost_round=params['n_estimators'],\n",
    "                      early_stopping_rounds=early_stopping_rounds,\n",
    "                      stratified=False)\n",
    "    \n",
    "    # Setting optimal number of estimators\n",
    "    key = list(cvresult.keys())[0]\n",
    "    n_rounds_optimal = len(cvresult[key])\n",
    "    model.set_params(n_estimators=n_rounds_optimal)\n",
    "    \n",
    "    print(cvresult[key][-1])\n",
    "    print(f\"n_estimators:{n_rounds_optimal}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:41:56.009220Z",
     "start_time": "2020-04-08T16:41:56.004212Z"
    }
   },
   "outputs": [],
   "source": [
    "def grid_search(estimator, X, y, params, scoring, cv=4, random=True,\n",
    "                n_iter=150, n_jobs=6):\n",
    "    \n",
    "    if random:\n",
    "        random_search = RandomizedSearchCV(estimator, \n",
    "                                           param_distributions=params,\n",
    "                                           n_iter=n_iter, n_jobs=n_jobs, \n",
    "                                           cv=cv, scoring=scoring,\n",
    "                                           verbose=3, random_state=340)\n",
    "    \n",
    "    else:\n",
    "        random_search = GridSearchCV(estimator, param_grid=params, \n",
    "                                       n_jobs=n_jobs, cv=cv,\n",
    "                                       scoring='neg_mean_absolute_error',\n",
    "                                       verbose=3)\n",
    "\n",
    "    random_search.fit(X, y)\n",
    "    \n",
    "    return random_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM uses leaf-wise tree growth algorithm. For this reason, limiting the number of leaves instead of max_depth might be more appropriate. Like we did when tuning XGBoost, we will start by finding the number of estimators using cross validation and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:38:51.033030Z",
     "start_time": "2020-04-08T16:38:51.029030Z"
    }
   },
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(learning_rate=0.1, n_estimators=1000,\n",
    "                          num_leaves=31, min_child_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:38:54.060556Z",
     "start_time": "2020-04-08T16:38:51.956560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12324168104250295\n",
      "n_estimators:74\n"
     ]
    }
   ],
   "source": [
    "model = find_nrounds_lgb(model, train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we find the **number of leaves**. The dataset is quite small, so we will start searching on low values. Let's vary **maximum depth** as well and see if that helps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:57:08.758988Z",
     "start_time": "2020-04-08T16:56:46.165571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 80 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=6)]: Done 276 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=6)]: Done 320 out of 320 | elapsed:   22.3s finished\n"
     ]
    }
   ],
   "source": [
    "param_test = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'num_leaves':range(1,41,2)\n",
    "}\n",
    "\n",
    "results = grid_search(model, train, y, param_test,\n",
    "                      scoring='neg_mean_squared_error', \n",
    "                      random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:57:20.621440Z",
     "start_time": "2020-04-08T16:57:20.614441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.08009788273854973\n",
      "{'max_depth': 9, 'num_leaves': 13}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {-results.best_score_}\")\n",
    "print(results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:57:27.636126Z",
     "start_time": "2020-04-08T16:57:27.631127Z"
    }
   },
   "outputs": [],
   "source": [
    "model.set_params(**results.best_params_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of min_child_weight like in XGBoost, let's tune **min_data_in_leaf**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:58:50.964610Z",
     "start_time": "2020-04-08T16:58:46.492645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 19 candidates, totalling 76 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=6)]: Done  76 out of  76 | elapsed:    4.2s finished\n"
     ]
    }
   ],
   "source": [
    "param_test = {\n",
    " 'min_data_in_leaf':range(5,100,5)\n",
    "}\n",
    "\n",
    "results = grid_search(model, train, y, param_test,\n",
    "                      scoring='neg_mean_squared_error', \n",
    "                      random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:58:53.170891Z",
     "start_time": "2020-04-08T16:58:53.164893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.08009788273854973\n",
      "{'min_data_in_leaf': 20}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {-results.best_score_}\")\n",
    "print(results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:58:58.716292Z",
     "start_time": "2020-04-08T16:58:58.712289Z"
    }
   },
   "outputs": [],
   "source": [
    "model.set_params(**results.best_params_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike XGBoost, we don't have a **gamma** parameter on LightGBM. The similar parameter here is **min_split_gain**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:59:37.726537Z",
     "start_time": "2020-04-08T16:59:30.352378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 51 candidates, totalling 204 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=6)]: Done 204 out of 204 | elapsed:    7.1s finished\n"
     ]
    }
   ],
   "source": [
    "param_test = {\n",
    " 'min_split_gain':[i/10.0 for i in range(0,51)]\n",
    "}\n",
    "\n",
    "results = grid_search(model, train, y, param_test,\n",
    "                      scoring='neg_mean_squared_error', \n",
    "                      random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T16:59:38.876032Z",
     "start_time": "2020-04-08T16:59:38.870029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.08009788273854973\n",
      "{'min_split_gain': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {-results.best_score_}\")\n",
    "print(results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:00:15.844318Z",
     "start_time": "2020-04-08T17:00:15.839318Z"
    }
   },
   "outputs": [],
   "source": [
    "model.set_params(**results.best_params_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:01:00.327627Z",
     "start_time": "2020-04-08T17:01:00.319636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.1, max_depth=9,\n",
       "              min_child_samples=20, min_child_weight=1, min_data_in_leaf=20,\n",
       "              min_split_gain=0.0, n_estimators=74, n_jobs=-1, num_leaves=13,\n",
       "              objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
       "              silent=True, subsample=1.0, subsample_for_bin=200000,\n",
       "              subsample_freq=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's **colsample_bytree** and **subsample**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:02:56.391472Z",
     "start_time": "2020-04-08T17:02:48.062474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 45 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=6)]: Done 180 out of 180 | elapsed:    8.1s finished\n"
     ]
    }
   ],
   "source": [
    "param_test = {\n",
    " 'subsample':[i/10.0 for i in range(5,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(1,10)]\n",
    "}\n",
    "\n",
    "results = grid_search(model, train, y, param_test,\n",
    "                      scoring='neg_mean_squared_error', \n",
    "                      random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:02:57.815505Z",
     "start_time": "2020-04-08T17:02:57.810471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.07821223535152377\n",
      "{'colsample_bytree': 0.3, 'subsample': 0.5}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {-results.best_score_}\")\n",
    "print(results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:03:03.818787Z",
     "start_time": "2020-04-08T17:03:03.813789Z"
    }
   },
   "outputs": [],
   "source": [
    "model.set_params(**results.best_params_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the learning_rate and increasing the number of estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:05:41.602325Z",
     "start_time": "2020-04-08T17:05:41.598325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=0.3,\n",
       "              importance_type='split', learning_rate=0.01, max_depth=9,\n",
       "              min_child_samples=20, min_child_weight=1, min_data_in_leaf=20,\n",
       "              min_split_gain=0.0, n_estimators=720, n_jobs=-1, num_leaves=13,\n",
       "              objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
       "              silent=True, subsample=0.5, subsample_for_bin=200000,\n",
       "              subsample_freq=0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.set_params(**{'learning_rate':0.01, 'n_estimators':720})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluating the final performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:05:51.140526Z",
     "start_time": "2020-04-08T17:05:43.653526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11346970518757978"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfolds = KFold(n_splits=10)\n",
    "np.mean(np.sqrt(-cross_val_score(model, train, y, \n",
    "                                 scoring=\"neg_mean_squared_error\",\n",
    "                                 cv=kfolds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/competitive-data-science/lecture/75oIn/catboost-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization (Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:29:21.823214Z",
     "start_time": "2020-04-08T17:29:20.494109Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nrounds_lgb(model, X, y, metrics='rmse',\n",
    "                     cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    lgbtrain = lgb.Dataset(X, y)\n",
    "    \n",
    "    params = model.get_params()\n",
    "    \n",
    "    # ATTENTION: we need to set stratified to False for regression\n",
    "    cvresult = lgb.cv(params, lgbtrain, metrics=metrics, \n",
    "                      num_boost_round=params['n_estimators'],\n",
    "                      early_stopping_rounds=early_stopping_rounds,\n",
    "                      stratified=False)\n",
    "    \n",
    "    # Setting optimal number of estimators\n",
    "    key = list(cvresult.keys())[0]\n",
    "    n_rounds_optimal = len(cvresult[key])\n",
    "    model.set_params(n_estimators=n_rounds_optimal)\n",
    "    \n",
    "    print(cvresult[key][-1])\n",
    "    print(f\"n_estimators:{n_rounds_optimal}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(estimator, X, y, params, scoring, cv=4, random=True,\n",
    "                n_iter=150, n_jobs=6):\n",
    "    \n",
    "    if random:\n",
    "        random_search = RandomizedSearchCV(estimator, \n",
    "                                           param_distributions=params,\n",
    "                                           n_iter=n_iter, n_jobs=n_jobs, \n",
    "                                           cv=cv, scoring=scoring,\n",
    "                                           verbose=3, random_state=340)\n",
    "    \n",
    "    else:\n",
    "        random_search = GridSearchCV(estimator, param_grid=params, \n",
    "                                       n_jobs=n_jobs, cv=cv,\n",
    "                                       scoring='neg_mean_absolute_error',\n",
    "                                       verbose=3)\n",
    "\n",
    "    random_search.fit(X, y)\n",
    "    \n",
    "    return random_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:30:53.359466Z",
     "start_time": "2020-04-08T17:30:53.353430Z"
    }
   },
   "source": [
    "Loading Preprocessed Data (check the house prices notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:31:03.351030Z",
     "start_time": "2020-04-08T17:31:03.287925Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/houses_preprocessed_train.csv').iloc[:, 1:]\n",
    "y = pd.read_csv('./data/houses_log_y.csv')['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the base model and finding the number of estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:31:18.365897Z",
     "start_time": "2020-04-08T17:31:18.360895Z"
    }
   },
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(learning_rate=0.1, n_estimators=1000,\n",
    "                          num_leaves=31, min_child_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T17:31:21.010752Z",
     "start_time": "2020-04-08T17:31:18.890796Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USUARIO\\Anaconda3\\envs\\dl\\lib\\site-packages\\lightgbm\\engine.py:503: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\USUARIO\\Anaconda3\\envs\\dl\\lib\\site-packages\\lightgbm\\basic.py:842: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
      "Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  .format(key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12324168104250295\n",
      "n_estimators:74\n"
     ]
    }
   ],
   "source": [
    "model = find_nrounds_lgb(model, train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Grid for Random Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:09:36.857797Z",
     "start_time": "2020-04-08T18:03:13.105617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 500 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=6)]: Done 116 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=6)]: Done 276 tasks      | elapsed:   46.0s\n",
      "[Parallel(n_jobs=6)]: Done 500 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=6)]: Done 788 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=6)]: Done 1140 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=6)]: Done 1556 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=6)]: Done 2000 out of 2000 | elapsed:  6.4min finished\n"
     ]
    }
   ],
   "source": [
    "param_test = {\n",
    " 'num_leaves':range(2,42,2),\n",
    " 'subsample': np.linspace(0.5, 1, 100),\n",
    " 'colsample_bytree': np.linspace(0.5, 1, 100),\n",
    " 'min_data_in_leaf':range(4,52,2),\n",
    " 'min_split_gain':np.linspace(0.0, 0.5, 100)\n",
    "}\n",
    "\n",
    "results = grid_search(model, train, y, param_test,\n",
    "                      scoring='neg_mean_squared_error', \n",
    "                      random=True, n_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:11:47.130005Z",
     "start_time": "2020-04-08T18:11:47.125012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.013705892792869795\n",
      "{'subsample': 0.6212121212121212, 'num_leaves': 34, 'min_split_gain': 0.005050505050505051, 'min_data_in_leaf': 34, 'colsample_bytree': 0.5050505050505051}\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {-results.best_score_}\")\n",
    "print(results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using best hyperparameters, reducing learning rate and increasing n_estimators proportionally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:11:49.054989Z",
     "start_time": "2020-04-08T18:11:49.050956Z"
    }
   },
   "outputs": [],
   "source": [
    "model.set_params(**results.best_params_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:11:50.178471Z",
     "start_time": "2020-04-08T18:11:50.172511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
       "              colsample_bytree=0.5050505050505051, importance_type='split',\n",
       "              learning_rate=0.01, max_depth=-1, min_child_samples=20,\n",
       "              min_child_weight=1, min_data_in_leaf=34,\n",
       "              min_split_gain=0.005050505050505051, n_estimators=740, n_jobs=-1,\n",
       "              num_leaves=34, objective=None, random_state=None, reg_alpha=0.0,\n",
       "              reg_lambda=0.0, silent=True, subsample=0.6212121212121212,\n",
       "              subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.set_params(**{'learning_rate':0.01, 'n_estimators':740})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final model performance (very close, but slightly worse than the sequential approach):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:12:02.902331Z",
     "start_time": "2020-04-08T18:11:51.861339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11596549007755608"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfolds = KFold(n_splits=10)\n",
    "np.mean(np.sqrt(-cross_val_score(model, train, y, \n",
    "                                 scoring=\"neg_mean_squared_error\",\n",
    "                                 cv=kfolds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization (Bayesian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://roamanalytics.com/2016/09/15/optimizing-the-hyperparameter-of-which-hyperparameter-optimizer-to-use/  \n",
    "\n",
    "https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf   \n",
    "\n",
    "https://github.com/lmassaron/kaggledays-2019-gbdt   \n",
    "\n",
    "https://www.quora.com/What-methods-do-you-prefer-when-performing-hyperparameter-optimization\n",
    "\n",
    "https://github.com/zygmuntz/hyperband"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
