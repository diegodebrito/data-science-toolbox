{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to check for:  \n",
    "- Inconsistent column names (capitalized, spaces, symbols)\n",
    "- Duplicate rows, untidy data  \n",
    "- Wrong variable types  \n",
    "- Cardinality of categorical variables  \n",
    "- Distribution of numerical feature / **outliers**  \n",
    "- Distribution / correlation of missing values  \n",
    "- Potential relationships  \n",
    "\n",
    "Types of Plots:\n",
    "- **Quantitative variables (univariate)**: histrograms, density plots (independent of bin size), boxplots (good for outliers)\n",
    "- **Categorical variables (univariate)**: frequency count and countplot.\n",
    "- **Quantitative v. Quantitative**: correlation matrix/heatmap, scatterplot, sns.jointplot (fancy scatter with histogram), scatterplot matrix\n",
    "- **Quantitative v. Categorical**: scatterplots with different colors, compare boxplots/violinplots side by side. Very useful: see example with catplot with two categorical dimensions.\n",
    "- **Categorical v. Categorical**: countplot + hue, contingency tables\n",
    "\n",
    "Some techniques:\n",
    "- df.head(), df.tail(), df.shape, df.columns\n",
    "- df.dtypes, df.info(), df.describe()\n",
    "- Frequency Count (good for categorical): .value_counts(dropna=False)\n",
    "- Summary Statistics (good for outliers): .describe()\n",
    "- bar plots for discrete data:  (df[col].value_counts().plot.bar())\n",
    "- t-SNE\n",
    "    \n",
    "Some tips/suggestions:\n",
    "- sns.set() make the plots look better\n",
    "- Seaborn plots return an Axes object. We can use matplotlib syntax to add titles, change axis, etc.\n",
    "- use masks for filtering (more organized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidy Data Rules:  \n",
    "1) Columns represent separate variabes  \n",
    "2) Rows represent individual observations  \n",
    "3) Observational units form tables   \n",
    "\n",
    "\n",
    "- Check for data types and unique values: any categorical column \"disguised\" as numeric? Numerical as strings?  \n",
    "\n",
    "Some useful techniques:  \n",
    "- **pd.melt(...)**: useful when columns represent similar information (imagine one column per country for GDP information, for example). pd.pivot_table(...) does the opposite transformation.  \n",
    "- **.str methods** and other methods: select/process strings in a vectorized fashion. Powerful: **re + .str.contains(pattern)**  \n",
    "- Use **.astype('float32')** to change data types. Very useful for constructing feature interactions.   \n",
    "- **.apply + custom function** is very flexible, but slow. Using pandas vectorized methods is faster (like .str methods). Check examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Be careful with **data leakage** for imputation (use the train set or cross validation). \n",
    "- **Mean or Median imputation**: usually median is better. Both can severely distort the distribution and artificially increase the number of outliers (by imputing median/mean, we reduce the variance). Good practice with imputation: creating **flag** variables.  \n",
    "\n",
    "\n",
    "Some interesting options on sklearn:  \n",
    "- **IterativeImputer**: uses a model, like BayesianRidge to estimate the missing values. Very flexible.\n",
    "- **KNNImputer**: similar to the above, but limited to KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a **production** environment, be careful about **unseen categories**. The encoding needs to be integrated to the pipeline. Alternatives: dropping data with unseen categories, encode as rare category or as missing value.  \n",
    "- **Caution**: one-hot encoding is usually really bad for tree-based models, specially with high cardinality. Avoid if possible.\n",
    "- **Frequency encoding**: be careful with leakage and labels with the same frequency.  \n",
    "- **Target encoding**: you can try **mean, median, std**. Be extra careful with leakage here. Use cross validation and oof predictions.\n",
    "- **Probability Ratio**: P(1)/P(0) for each label.  \n",
    "- **High cardinality**: besides using the techniques above, you can try to bin into smaller groups. Optimal way: order by target mean and break into groups following that order (LightGBM does this). Option: bundle of **rare** categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For **non-Gaussian/Skewed** variables: log, reciprocal (1/x), square root, exponential, Box-Cox / Yeo-Johnson (see below)\n",
    "- **MaxAbsScaler**: robust to very small standard deviation and preserves zero entries on sparse data.  \n",
    "- **RobustScaler**: removes the median ans scales according to quantile range (1st and 3rd).  \n",
    "- Mapping to uniform distribution (QuantileTransformer)\n",
    "- Mapping to Gaussian distribution: stabilizes variance and reduces skewness. PowerTransformer (Yeo-Johnson or **Box-Cox**). Box-Cox only works for strictly positive data. \n",
    "\n",
    "**Sklearn**  \n",
    "- from sklearn.preprocessing import StandardScaler, MinMaxSacaler, MaxAbsScaler, RobustScaler, QuantileTransformer  \n",
    "- QuantileTransformer(random_state=0): maps to uniform distribution between 0 and 1. We can map to the normal distribution by setting output_distribution='normal'.\n",
    "- pt = preprocessing.PowerTransformation(method=**'box-cox'**, standardize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bag of words ignores the order the words appear on a document. These will need to be stored using sparse matrices.  \n",
    "- **CountVectorizer** provides raw term frequencies. For some estimators (Bernoulli Naive Bayes) it might be better to set binary=True option.    \n",
    "- Words that occur in many documents are not very useful for classification. We can adjust for this effect by using term frequency-inverse document frequency (**tf-idf**).  \n",
    "- **n-grams** instead of 1-gram can be useful for similar phrases: 'This is fun', 'is this fun?'. 'is this' would appear on the second phrase (maybe we should include the question mark?)  \n",
    "- Choosing which **stop words** to use is not a trivial problem. Words that are uninformative in one problem can be very important in a different problem. Countvectorizer has the option 'english' stop words. Removing stop words is more useful when using raw counting (tfidf downplay repeated words anyway).  \n",
    "- Check the documentation for useful tips about **encoding**.  \n",
    "- For word **stemming** (transforming a word into its root form), use the NLTK package. Stemming can result in weird words (like thu from thus). **Lemmanization** provides more natural forms (lemmas), but does not seem to improve predictions (ans is also more computationally expensive).\n",
    "\n",
    "- We need to hold the complete vocabulary on memory for CountVectorizer and/or inverse document frequencies. When doing **online learning**, we can use a **HashingVectorizer** instead.\n",
    "\n",
    "**Sklearn**  \n",
    "- from sklearn.feature_extraction.text import **CountVectorizer()**. ngram_range=(1, 1) is the default.  \n",
    "- Setting **different analyzers** with ngram_range argument. CountVectorizer(analyzer='char_wb', ngram_range=(2,2)) will create n-grams from characters inside word boundaries padded with spaces. The 'char' analyzer creates n-grams that span across words.\n",
    "- from sklearn.feature_extraction.text import **TfidfTransformer** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T23:13:15.150920Z",
     "start_time": "2020-04-07T23:13:15.136944Z"
    }
   },
   "source": [
    "Things to try:\n",
    "- Ratios, total distance, difference, sum, interaction, fractional part of prices\n",
    "- Concatenation of string features\n",
    "2. Target Encoding (be careful with spilling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Useful tools from sklearn: **VarianceThreshold** and **SelectFrom Model**. The latter works with models that have coef_ or fature_importnces_ attribute.\n",
    "\n",
    "**Permutation Feature Importance**  \n",
    "- We can evaluate permutation feature importance on the training set or the validation set (for generalization error).  \n",
    "- Features that are important on the training set but not on the validation set could be causing the model to overfit.  \n",
    "- Feature deemed non-important for some model with low predictive performance coul be highly predictive for a model that generalizes better -> evaluate how good a model is with cross-validation before evaluating feature importance.  \n",
    "- Imputiry-based feature importance from tree based models give importance to features that may not be predictive on unseen data.   \n",
    "- We can use different scoring methods with permutationf feature importance.\n",
    "- Tip: cluster features that are correlated and keep one feature from each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection / Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-validation**\n",
    "- Even when using a validation set, our model choice can depend on the random choice for the pair of (train, validation). It might be better to do cross-validation instead.  \n",
    "\n",
    "\n",
    "- For **large imbalance** in the distribution of classes, use stratification. If the order of classes is not arbitraty, use **shuffling** (you can use the option on KFold to shuffle the indeces only).\n",
    "\n",
    "\n",
    "- **Important**: consider doing cross validation on some preprocessing steps as well (use pipelines)\n",
    "\n",
    "\n",
    "- from sklearn.model_selection import **cross_val_score** (we can give an integer to cv or a cross-validation generator)   \n",
    "  scores = cross_val_score(clf, X, y, cv=5, scoring=...)  \n",
    "  See https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter  \n",
    "  We can use different cross validation strategies (below) or a custom iterator (see code section):  \n",
    "  from sklearn.model_selection import ShuffleSplit  \n",
    "  cv = ShuffleSplit(...)  \n",
    "  cross_val_sore(clf, X, y, cv=cv)   \n",
    "  \n",
    "  \n",
    "- **cross_validate** returns more data and accepts a list of metrics;  \n",
    "  **cross_val_predict** returns cross-validated predictions\n",
    "  \n",
    "  \n",
    "- **Cross validation iterators**: KFold, RepeatedKFold, Leave One Out (LOO), eave P Out (LPO), ShuffleSplit, StratifiedKFold, RepeatedStratifiedKFold, StratifiedShuffleSplit. More advanced: iterators for groups.\n",
    "- **Custom fold iterators**: check notebook with example.\n",
    "\n",
    "\n",
    "- **Time Series Split**: returns first k fold as the training set and the k+1 as the test set (successive training sets are supersets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter Search**\n",
    "- Grid of parameters are very flexible: you can vary models and preprocessing methods as well, not just parameters. Check notebook for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pipeline for chaining multiple transformers at the same time\n",
    "\n",
    "\n",
    "- Applying **transformations in parallel** and concatenating the results (useful to combine several feature extraction methods):  \n",
    "  from sklearn.pipeline import **FeatureUnion**  \n",
    "  transformer = FeatureUnion(transformer_list = [('name', SimpleImputer(...)), ...])\n",
    "  \n",
    "  \n",
    "  \n",
    "- Quick way to construct a pipeline from estimators:  \n",
    "  from sklearn.pipeline import **make_pipeline**  \n",
    "  make_pipeline(transformer, DecisionTreeClassifier())  \n",
    "  obs: we can't name the estimators using the function  \n",
    "  \n",
    "  \n",
    "- Extremely important: **custom sklearn transformers** (check code section). Use map + dictionaries extensively.   \n",
    "\n",
    "\n",
    "- Very useful command: pipeline.get_params().keys()\n",
    "\n",
    "\n",
    "\n",
    "- Excellent tool/tips: **make_column_selector**, using a transformer on the remainder argument, **make_column_transformer**\n",
    "\n",
    "\n",
    "- Should we select features before or during the pipeline? Trade-off between flexibility and more code. If the data does not change a lot, use pre-selected features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "191.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
